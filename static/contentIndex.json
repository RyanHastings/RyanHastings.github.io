{"index":{"slug":"index","filePath":"index.md","title":"My Portfolio","links":["Classification-Model-With-Numerical-Features"],"tags":[],"content":"\nA data scientist is someone who is better at software than a statistician and better at stats than a software engineer. (Attribution Unknown)\n\nIn 2019, I left research meteorology to enter into data science. No longer interested in academia, I sought to apply my skillsets of analyzing data, formulating hypotheses regarding that data, and testing those hypotheses to a wider array of fields. This has been a thoroughly enjoyable career pivot, as I have worked on a variety of different problems, which so far have included behavioral economics, epidemiology, and record linkage. All of the work I did is confidential to the government agencies (and often covered by HIPPA or FERPA), but here I will showcase some of the types of work I did. Within this portfolio I will walk through some common data science problems, as well as explore some interesting questions and techniques that I do not commonly see.\nAt this point I have only completed one of the portfolio projects:\n\nClassification Model With Numerical Features\n\nPhilosophy on the use of LLMs for coding. Large language models are here, and anyone not willing to use them for help on coding problems is not using a potentially powerful tool. In my own experience, I have found ChatGPT helpful for a few things: If I’m particularly stuck on a problem, if I foresee that solving a problem will take longer than is reasonable for getting a project done with some celerity, or if I have come up with a solution but find it clunky and am curious if there is a more elegant solution (and usually there’s not, at least not one that the LLM can come up with).\nHowever, I do not “vibe code.” Using an LLM for more than a few hundred lines of code at most is asking for trouble. They will frequently give flawed code, and it will need to be adjusted for successful implementation into whatever project I am working on. Plus, there’s the potential problem of the degradation of cognitive function if cognitive tasks are offloaded onto an algorithm, which could end up being to our long term detriment.\nAs for as for analysis, I find the LLMs I’ve used to be hit or miss. They fail on sophisticated statistical analyses, but often they also know of techniques I am not familiar with, and thus offer helpful direction. As with coding, any analysis suggestions they make need to be validated by a human.\nFuture Work will include the exploration of classification models that mix numerical and categorical features, strategies for handling missing values, and using SHAP values for investigating interactions in a model agnostic way."},"portfolio-1/1-construction":{"slug":"portfolio-1/1-construction","filePath":"portfolio-1/1-construction.md","title":"1. Construction of the Data Set","links":[],"tags":[],"content":"\nImport Libraries\nCreate dataset\n\nImport Libraries\nAs always, we begin by importing necessary functions from various libraries: numpy, pandas, matplotlib, seaborn, scipy, shap, and scikit-learn.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import spearmanr\nimport shap\n \nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import mutual_info_regression, SelectFromModel\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, \\\n    brier_score_loss, log_loss, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.base import clone\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.frozen import FrozenEstimator\nCreate dataset\nNext we create the dataset. It will have 50,000 samples, thirty features, and be associated with a binary class. 0.7 of the samples which will be negative (0) and the remainder positive (1). 20 of the features will be informative and 5 redundant.\n# create data set\nX, y = make_classification(\n    n_samples = 50000,\n    n_features = 30,\n    n_classes = 2,\n    n_informative = 20,\n    n_redundant = 5,\n    shuffle = True,\n    random_state = 42,\n    n_clusters_per_class = 1,\n    class_sep = 1,\n    weights = [0.7,]\n)\n \n# concantenate them into one frame and create column names that are simply X plus the number of column position\ndata = pd.DataFrame(X, columns = [&#039;X&#039; + str(n) for n in range(30)]).assign(target=y)\n \n# take a look at the data frame\ndata.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      X0\n      X1\n      X2\n      X3\n      X4\n      X5\n      X6\n      X7\n      X8\n      X9\n      ...\n      X21\n      X22\n      X23\n      X24\n      X25\n      X26\n      X27\n      X28\n      X29\n      target\n    \n  \n  \n    \n      0\n      0.107030\n      5.857486\n      -0.501960\n      -0.630735\n      0.547076\n      -4.274163\n      15.300989\n      4.762018\n      3.399235\n      -1.875450\n      ...\n      -1.614864\n      2.706797\n      0.636519\n      -5.180686\n      5.917957\n      2.218638\n      3.705592\n      -2.352288\n      5.437839\n      0\n    \n    \n      1\n      -0.617422\n      -2.260785\n      -0.920099\n      4.851533\n      -0.975667\n      0.393628\n      1.223156\n      -0.876361\n      -0.552878\n      0.817603\n      ...\n      3.005481\n      -2.238610\n      -0.160044\n      -1.743732\n      -5.538825\n      3.567385\n      -5.170720\n      0.283220\n      -0.612102\n      1\n    \n    \n      2\n      0.923485\n      0.596967\n      -4.973955\n      -1.475220\n      -5.166408\n      2.858951\n      9.810482\n      3.263809\n      -0.070150\n      -0.952388\n      ...\n      -3.032525\n      -1.037887\n      1.729179\n      -0.675174\n      3.160159\n      1.899279\n      -1.805782\n      -1.469503\n      1.952417\n      1\n    \n    \n      3\n      1.132601\n      -0.817862\n      1.830852\n      -8.783676\n      6.366377\n      -1.983822\n      4.238954\n      -0.506094\n      0.738517\n      -4.967117\n      ...\n      -5.471895\n      -4.347002\n      -1.605614\n      0.099890\n      1.513436\n      -1.348966\n      -1.273202\n      -1.341054\n      -3.541808\n      1\n    \n    \n      4\n      1.564079\n      -2.209570\n      0.011249\n      -2.332179\n      -1.424068\n      2.019029\n      5.746230\n      -1.138455\n      -3.561440\n      -2.635148\n      ...\n      -2.375369\n      2.071340\n      0.251526\n      1.704671\n      0.401561\n      -2.491117\n      -0.755343\n      -0.779528\n      4.030070\n      0\n    \n  \n\n5 rows × 31 columns\n"},"portfolio-1/conclusion":{"slug":"portfolio-1/conclusion","filePath":"portfolio-1/conclusion.md","title":"5. Conclusion","links":[],"tags":[],"content":"Here I have walked through the skeleton of the standard process for setting up a machine learning model. Along the way I introduced some concepts I seldom see discussed elsewhere, such as the mutual information coefficient and the use of feature selection using error contribution rather than prediction contribution. Our final result was that a multilayer perceptron with recursive feature selection based on error contribution was clearly superior.\nIt might be tempting for the naive data scientist to just assume the more complex neural networks or deep learning systems would always automatically be better. But even within this project, we saw that the simplest classification model, logistic regression, performed significantly better than the more complex ensemble models. It can never be a foregone conclusion that the complicated model will automatically be better, and with any problem multiple models should be investigated. For example, when I was analyzing infant mortality in Indiana, the random forest performed best. In addition, simpler is generally going to be better for the simple reason that simple models are easier to understand and interpret, in addition to having a lower computational cost. But the choice should always be guided by the requirements of the data more than anything else."},"portfolio-1/data-exploration":{"slug":"portfolio-1/data-exploration","filePath":"portfolio-1/data-exploration.md","title":"2. Data Exploration","links":[],"tags":[],"content":"\nInitial examination of data\nMissing Values\nDistribution of Independent Variables\nAssociations Between Features\nThe Use of the Mutual Information Coefficient\n\nInitial examination of data\nFirst, I will look at the number of rows (which are by design, so I actually already know), and the distribution of the categories.\n# get the length of the data frame\nnrows = len(data)\n \n# number and proportion for target=1\nntarget1 = data[&#039;target&#039;].sum()\nptarget1 = ntarget1/nrows\n \n# number and proportion for target=0\nntarget0 = nrows-ntarget1\nptarget0 = 1-ptarget1\n \n# present results\nprint(f&quot;The total number of rows is {nrows}. Of those, {ntarget1} are positive, {ntarget0} are negative.&quot;+ \n      f&quot;\\nThe proportion of positive cases {ptarget1:.03} and negative cases {ptarget0:.03}.&quot;)\n \n# pie chart\nfig,ax = plt.subplots()\nax.pie(x=[ptarget1,ptarget0],labels=[&#039;positive&#039;,&#039;negative&#039;],autopct=&#039;%1.1f%%&#039;);\nplt.show()\nThe total number of rows is 50000. Of those, 15073 are positive, 34927 are negative.\nThe proportion of positive cases 0.301 and negative cases 0.699.\n\n\n\n\n\nMissing Values\nI also want to see how many missing values there are.\n# this loop will print the name of any column with a nonzero sum of missing values along with the number of missing values\nno_missing_flag = True\nfor colname in data.columns:\n    n_nans = data[colname].isna().sum()\n    if n_nans &gt; 0:\n        print(f&quot;{colname} has {n_nans} missing values.&quot;)\n        no_missing_flag = False\nif no_missing_flag:\n    print(&quot;There are no missing values.&quot;)\nThere are no missing values.\n\nBecause this is a synthetic dataset, there are no missing values. Were there missing values, I would then need further analysis to determine whether to drop rows or columns with missing data or try some method of imputation (which will be demonstrated in a future addition to this portfolio).\nDistribution of Independent Variables\nThese data are all numerical, so to get a look at the distributions of feature values I will simply use the describe method. On a real dataset I would use domain knowledge to take a closer look at features I think might be relevant.\ndata.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      X0\n      X1\n      X2\n      X3\n      X4\n      X5\n      X6\n      X7\n      X8\n      X9\n      ...\n      X21\n      X22\n      X23\n      X24\n      X25\n      X26\n      X27\n      X28\n      X29\n      target\n    \n  \n  \n    \n      count\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      ...\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n      50000.000000\n    \n    \n      mean\n      -0.009305\n      0.385435\n      0.402292\n      0.067548\n      -2.437292\n      -0.374888\n      4.402445\n      0.984402\n      -0.407140\n      -0.400710\n      ...\n      -0.992325\n      1.010425\n      -0.416586\n      0.388522\n      0.380183\n      -0.995731\n      0.386466\n      -0.001563\n      1.014605\n      0.301460\n    \n    \n      std\n      1.001876\n      2.877833\n      2.704703\n      7.284647\n      6.166970\n      2.724958\n      7.715400\n      2.475133\n      2.543759\n      2.350943\n      ...\n      2.801943\n      2.810665\n      2.779655\n      2.893455\n      2.792027\n      2.425633\n      2.725618\n      0.995862\n      2.523641\n      0.458897\n    \n    \n      min\n      -4.309958\n      -12.576973\n      -10.434212\n      -27.566633\n      -26.646586\n      -10.982415\n      -25.189695\n      -8.883930\n      -13.818169\n      -9.206762\n      ...\n      -12.265536\n      -10.008664\n      -12.440082\n      -12.672520\n      -11.012225\n      -14.302898\n      -10.511940\n      -4.034314\n      -9.444280\n      0.000000\n    \n    \n      25%\n      -0.685982\n      -1.538148\n      -1.450334\n      -4.871029\n      -6.573651\n      -2.206324\n      -0.901286\n      -0.677715\n      -2.133717\n      -2.012085\n      ...\n      -2.876197\n      -0.866984\n      -2.296212\n      -1.581324\n      -1.515726\n      -2.625107\n      -1.454356\n      -0.673433\n      -0.680536\n      0.000000\n    \n    \n      50%\n      -0.010349\n      0.395964\n      0.360203\n      -0.296250\n      -2.434660\n      -0.377266\n      4.297483\n      0.991522\n      -0.391489\n      -0.488600\n      ...\n      -0.991356\n      1.002126\n      -0.406314\n      0.390705\n      0.378676\n      -0.985660\n      0.403399\n      0.000029\n      1.016488\n      0.000000\n    \n    \n      75%\n      0.671657\n      2.341213\n      2.220133\n      4.671136\n      1.714638\n      1.451967\n      9.611936\n      2.639159\n      1.313721\n      1.122236\n      ...\n      0.878624\n      2.898819\n      1.486461\n      2.356106\n      2.277618\n      0.622938\n      2.236307\n      0.669239\n      2.713905\n      1.000000\n    \n    \n      max\n      4.014015\n      11.043349\n      12.134694\n      34.839229\n      23.487909\n      11.338587\n      40.424541\n      11.305053\n      10.613916\n      10.470961\n      ...\n      10.353978\n      12.980623\n      11.112211\n      13.285860\n      12.423742\n      10.259914\n      12.033986\n      4.152785\n      11.962137\n      1.000000\n    \n  \n\n8 rows × 31 columns\n\nAssociations Between Features\nNext I want to take a quick look at associations. While this might commonly be done with Pearson’s correlation coefficient, I prefer to use the Spearman. Pearson’s assumes a linear relationship, and thus can fail to reflect robust nonlinear associations. That is, there might be a very good nonlinear association but Pearson’s coefficient would show a weaker one. By using rank, Spearman only considers the mutual monotonicity of the variables.\nI will begin by looking only at the features.\n# set up the figures\nfg, ax = plt.subplots(1,1,figsize=(10,10))\n \n# create a correlation matrix\ncorr_matrix = data.iloc[:,:-1].corr(method=&#039;spearman&#039;)\n \n# create a heatmap\nax = sns.heatmap(np.abs(corr_matrix))\n \nplt.show()\n\nWe see some higher correlations between features (e.g. X13 and X16), as well as completely uncorrelated features (which was by design). One option we could now use is to try linear combinations of those highly correlated features. However, this should be handled by our feature selection methods before.\nThe Use of the Mutual Information Coefficient\nI would like more information than Spearman can provide for measuring the association between the features and the target variable. For that, I like the mutual information coefficient. That particular measure I found from An Undeservedly Forgotten Correlation Coefficient on Towards Data Science. In short, the mutual information is the relative entropy between the joint distributions of two variables and the product of their marginal distributions. That is for probability p defined over variables X and Y with values x and y respectively with joint probability p(x,y) and marginal probabilities p(x) and p(y):\nI(X,Y) = \\int_X \\int_Y p(x,y) \\text{log}\\frac{p(x,y)}{p(x)p(y)}.\nThis is scaled to be between on the interval [0,1] through:\nR(X,Y) = [1 - \\text{exp}(-2*I(X,Y))]^\\frac{1}{2}.\nThis is the mutual information coefficient (MIC), and it is significantly more informative than either Pearson or Spearman. We can regard it as telling us how much information we get about one variable when we know the other variable. An example of its utility in the above article uses a plot that is a ring in the plane. There clearly is a strong relationship between the x and y variables, but traditional correlation coefficients show virtually no relationship. The MIC, on the other hand, reflects the actual strong association. Note it does take some time to compute so it is not efficient to use on a large data set. I will use it to look at relationships between the features and the target variable, to get an initial idea of the effect size from each individual feature.\n# compute the mutual information coefficient\nMI = mutual_info_regression(data.iloc[:,:-1],data[&#039;target&#039;])\nR = np.sqrt(1-np.exp(-2*MI))\n \n# create a quick dataframe fro ease of plotting\ndf_R = pd.DataFrame({&#039;feature&#039;:data.columns[:-1],&#039;MIC&#039;:R}).sort_values(&#039;MIC&#039;)\n \n# make a plot\nfg, ax = plt.subplots(1,1,figsize=(10,10));\n \nax.barh(y = df_R[&#039;feature&#039;], width = df_R[&#039;MIC&#039;]);\nax.set_title(&quot;Mutual Information Coefficients of Features and Target&quot;);\nax.set_xlabel(&quot;Mutual Information Coefficient&quot;);\nax.set_ylabel(&quot;Feature&quot;);\n \n\nWe see a number of features that are completely uncorrelated, so we can safely drop those.\ndata.drop(columns = df_R.loc[df_R[&#039;MIC&#039;]==0,&#039;feature&#039;], inplace = True)"},"portfolio-1/feature-selection":{"slug":"portfolio-1/feature-selection","filePath":"portfolio-1/feature-selection.md","title":"4. Feature Selection","links":[],"tags":[],"content":"Now, of course, the fewer features we need the better, both for the aesthetic of parsimony and the pragmatics of needing fewer computing cycles, so the next step is to do some form of feature or model selection.\nHere I will employ recursive feature elimination. This has a simple set of steps:\n\nTrain model with a set of features.\nUse some measure to determine feature importance.\nRemove the least important feature.\nGo to step 1 with the new feature set.\n\nThis can be done until a particular criteria is met. I find with these relatively small sets of features it can be helpful to simply remove them all until only one is left, and then select the best performing configuration after that.\n\nContribution to Error versus Contribution to Prediction\nSHAP values and probability predictions\nSHAP Values and Error Contribution\nA Recursive Feature Elimination Class\nComparing Prediction and Error Methods of RFE\n\nContribution to Error versus Contribution to Prediction\nThis is traditionally done by looking at what feature provides the least significant contribution to making a successful prediction. For example, this could be using the coefficients of a logistic regression model, or the feature_importance_ attribute of a tree-based model. However, I came across ‘Which Features Are Harmful For Your Classification Model?” by Samuele Mazzanti on Towards Data Science and the method intrigued me.\nThe idea here is not eliminating which feature is most helpful, but which feature is most harmful. That is, find which feature contributes most to the the model error rather than prediction. As presented in the article, this error-based feature is unquestionably superior, so naturally it seemed worth further exploration. The algorithm is the same as above, except “feature importance” is determined by this contribution to error. Hereafter I will refer to recursive feature elimination according to contribution to prediction as the “prediction method” and that according to the contribution to error as the “error method.”\nMazzanti uses SHAP values to determine both prediction and error contributions. I will follow that here. SHAP values have the advantage of being model agnostic, and are an easy way to determine how various features contribute to various predictions. Some complexity emerges because each feature will have a different SHAP value for each prediction. This means that the contribution a feature makes to prediction or error will depend on the specific prediction being made. (A future addition to this portfolio will examine how this enables SHAP values to examine interactions between features without necessitating the additional mathematical apparatus one might need in, say, a generalized linear model with interaction terms. This is a great deal of the work I did for the analysis of infant mortality at the Indiana Department of Public Health.)\nI will use the shap package to compute the SHAP values, as Mazzanti does. He explores a tree-based model and thus can use the shap.TreeExplainer class. I will use the generic shap.Explainer class, as I intend to look at four different models.\nFor these, the predicted probability is simply the sum of the SHAP values of each of the features plus a baseline value.\nUsing SHAP values, the contribution to prediction is pretty easy to find. It is simply the mean absolute value of the SHAP values for that feature.\nFor error, we need some loss metric. Again following Mazzanti, I am using a version of log loss or cross entropy. He proposed an “individual log loss,” which is computed for each prediction individually using the general log loss formula of\n-y\\text{log}(p)-(1-y)\\text{log}(1-p)\nfor predicted probability p that the actual class y is 1.\nSHAP values and probability predictions\nNow, in the article, Mazzanti claims that the sum of SHAP values is not equal to the predicted probability. He uses a sigmoid function to convert the sums into the predicted probability. I was surprised by this, because in my understanding that is exactly what SHAP values are supposed to sum to. Their value is they show quantitatively how each feature contributes to the prediction. In the case of a classification model, the SHAP value is the amount by which the probability is increased or decreased. In order to check this, I did some quick investigation. I had two specific questions:\n\nAre the predicted probabilities simply the straightforward sums of the SHAP values and the base values?\nMazzanti gives a method for finding what the model would predict if trained without that feature, which is simply substracting the SHAP value of that feature (before plugging the result into his sigmoid function). This seems pretty clearly inaccurate to me, but I also felt it worth checking.\n\nBelow I quickly:\n\nTrain a calibrated estimators on the full set.\nFind SHAP values and sum them.\nPredict probabilities using the predict_proba method.\nSubtract the SHAP values for the X3 columns (which above we saw having the highest correlation to the target) from the sum.\nTrain the calibrated estimators with the X3 feature removed.\nGet the predicted probabilities from predict_proba.\n\nWhile I had originally intended to do this for all of the estimators, it quickly became clear with only the logistic regression that Mazzanti’s premise was completely mistaken.\n \ndef get_shaps(X,estimator,feature_to_drop=-1):\n    # A quick function to compute shap value sums, predicted probabilities, a sigmoid based on the shaps\n    # in order to test Mazzanti&#039;s claims\n    X1000 = X.iloc[:500,]\n    explainer = shap.Explainer(lambda x: estimator.predict_proba(x)[:,1],X1000)\n    explained = explainer(X.iloc[:500,])\n    shap_values = explained.values\n    shap_values_sum = np.sum(shap_values,axis=1)+explained.base_values\n    if feature_to_drop&gt;-1:\n        shap_values_sum-=shap_values[:,feature_to_drop]\n    shap_sigmoid = 1/(1+np.exp(-shap_values_sum))\n    pred_probs = estimator.predict_proba(X.iloc[:500,])[:,1]\n \n    return shap_values_sum, pred_probs, shap_sigmoid\n \n# use the logistic regression as it is the quickest and simplest\nestimator = &#039;logistic regression&#039;\n \n# train on the full training set and calibrate with the validation set\ntemp1 = clone(estimators[estimator]).fit(X_train,y_train)\ntemp1 = CalibratedClassifierCV( FrozenEstimator(temp1) ).fit(X_val, y_val)\n \n# get the sums of the shap values and the predicted probabilities\nshap_values_all, pred_probs_all, shap_sigmoid_all = get_shaps(X_val,temp1)\n \n# get the sums of the shap values minus feature X3\nshap_values_less, pred_probs_less, shap_sigmoid_less = get_shaps(X_val,temp1,3)\n \n# the relative error of the probability predicted by the estimator and that predicted by summing the shap values\nd1 = (pred_probs_all-shap_values_all)/pred_probs_all\n \n# drop the X3 column and train a new estimator\ntemp2 = clone(estimators[estimator]).fit(X_train.drop(columns=&quot;X3&quot;),y_train)\ntemp2 = CalibratedClassifierCV( FrozenEstimator(temp2) ).fit(X_val.drop(columns=&quot;X3&quot;),y_val)\n \n# get the predicted probabilities from the estimator after X3 is dropped\npred_probs_less2 = temp2.predict_proba(X_val.iloc[:500,].drop(columns=&quot;X3&quot;))[:,1]\n \n# compare the shap values of the full model less that for X3 to the predicted probabilities after X3 is dropped\nd2a = pred_probs_less2 - shap_values_less\nd2b = (pred_probs_less2-shap_values_less)/pred_probs_less2\n \n# the same but with the sigmoid of the shap values of the full model less than for X3\nd3 = (pred_probs_less2-shap_sigmoid_less)/pred_probs_less2\n \n    \ndisplay(\n        pd.DataFrame(\n            {\n                &#039;predicted probs full model&#039;:pred_probs_all,\n                &#039;shap sums full model&#039;:shap_values_all,\n                &#039;relative error full model&#039;:d1,\n                &#039;predicted probs with feature removed&#039;:pred_probs_less2,\n                &#039;shap values with feature subtracted&#039;:shap_values_less,\n                &#039;sigmoid&#039;:shap_sigmoid_less,\n                &#039;absolute error of feature removal&#039;:d2a,\n                &#039;relative error of feature removal&#039;:d2b,\n                &#039;rel err of sigmoid&#039;:d3\n            }\n        ).describe()\n    )\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      predicted probs full model\n      shap sums full model\n      relative error full model\n      predicted probs with feature removed\n      shap values with feature subtracted\n      sigmoid\n      absolute error of feature removal\n      relative error of feature removal\n      rel err of sigmoid\n    \n  \n  \n    \n      count\n      500.000000\n      500.000000\n      5.000000e+02\n      500.000000\n      500.000000\n      500.000000\n      500.000000\n      500.000000\n      500.000000\n    \n    \n      mean\n      0.261552\n      0.261552\n      -7.461147e-14\n      0.261562\n      0.263971\n      0.561947\n      -0.002409\n      -144.891866\n      -2475.497736\n    \n    \n      std\n      0.413254\n      0.413254\n      1.047104e-12\n      0.413246\n      0.395773\n      0.091899\n      0.039179\n      1038.041554\n      12795.600220\n    \n    \n      min\n      0.000003\n      0.000003\n      -1.994995e-11\n      0.000003\n      -0.067246\n      0.483195\n      -0.105971\n      -21378.814052\n      -192176.325976\n    \n    \n      25%\n      0.000827\n      0.000827\n      -5.428578e-15\n      0.000823\n      0.014407\n      0.503602\n      -0.029115\n      -19.115360\n      -617.267819\n    \n    \n      50%\n      0.006178\n      0.006178\n      -1.979455e-16\n      0.006136\n      0.036663\n      0.509165\n      -0.008436\n      -0.359913\n      -80.743209\n    \n    \n      75%\n      0.611638\n      0.611638\n      2.658062e-15\n      0.611394\n      0.560319\n      0.636518\n      0.015195\n      0.064101\n      -0.026557\n    \n    \n      max\n      1.000000\n      1.000000\n      2.718276e-12\n      1.000000\n      1.104566\n      0.751115\n      0.189717\n      77.854396\n      0.294555\n    \n  \n\n\nAnd here we see Mazzanti is wrong. Simply summing the SHAP values (with the base value) is almost exactly what the estimator predicts with the predict_proba method. Removing the feature from the SHAP sum results in large errors relative to what predict_proba yields for a model with the feature removed. Using a sigmoid function results in a value that has no relevance to anything.\nSHAP Values and Error Contribution\nBut my curiosity did not end there. Is it possible that, despite the discrepancy in the predicted probability from a model trained without the feature and a model trained with the feature but with the feature’s SHAP value subtracted might still approximate the importance of the feature? I decided to take a look at it.\n# list to hold the error-contribution-based feature importance based on retraining the model\nfeature_importance_from_trained = []\n \n# list to hold the error-contribution-based feature importance based on substracting shap value\nfeature_importance_from_shap = []\n \n# get a numpy array for the use in log loss computations\ny = y_val.iloc[:500,].to_numpy()\n \ndef get_log_loss_vector(y,pred_probs,eps=1e-15):\n    # individual log loss\n    p = np.clip(pred_probs,eps,1-eps)\n    return -(y*np.log(p)+(1-y)*np.log(1-p))\n \n# get the individual log loss from the model trained wih all features\nlog_loss_vector_all = get_log_loss_vector(y,pred_probs_all)\n \n# loop through removing features\nfor i in range(X_train.shape[1]):\n \n    # get the probability from subtracting SHAP value of feature\n    shap_values_less, pred_probs_less, shap_sigmoid_less = get_shaps(X_val,temp1,i)\n \n    # get column name to drop\n    col_to_drop = X_train.columns[i]\n \n    # retrain model\n    temp2 = clone(estimators[estimator]).fit(X_train.drop(columns=col_to_drop),y_train)\n    temp2 = CalibratedClassifierCV( FrozenEstimator(temp2) ).fit(X_val.drop(columns=col_to_drop),y_val)\n \n    # get the log loss from the retrained model\n    temp_log_loss_vector = get_log_loss_vector(y,temp2.predict_proba(X_val.iloc[:500,].drop(columns=col_to_drop))[:,1])\n \n    # get the difference in log loss resulting from retrained model\n    log_loss_diff_trained = log_loss_vector_all - temp_log_loss_vector\n \n    # importance is the mean\n    feature_importance_from_trained.append(np.mean(log_loss_diff_trained))\n \n    # get the difference in log loss resulting from subtracting SHAP value\n    shap_log_loss_vector = get_log_loss_vector(y,shap_values_less)\n    log_loss_diff_shap = log_loss_vector_all - shap_log_loss_vector\n \n    # importance is the mean\n    feature_importance_from_shap.append(np.mean(log_loss_diff_shap))\n    \n# create a data frame real quick for easy plotting\nerror_contribution_frame = pd.DataFrame(\n    {\n        &#039;by training&#039;:feature_importance_from_trained,\n        &#039;by shap&#039;:feature_importance_from_shap\n    },\n    index = X_train.columns\n)\n \n# rescale log loss difference from training for plotting\nerror_contribution_frame[&#039;by training * 100&#039;] = error_contribution_frame[&#039;by training&#039;]*100\n \n# quick plot\nerror_contribution_frame[[&#039;by shap&#039;,&#039;by training * 100&#039;]].plot.barh() ;\n\nThe difference here requires no further formal investigation. Simply subtracting the SHAP value for the feature does not give a remotely comparable result to retraining the model without the feature.\nA Recursive Feature Elimination Class\nDespite this, I still found the underlying idea intriguing. While it would not be as simple as reproducing Mazzanti, there still might be value in trying this method of recursive feature elimination.\nBelow I create a class RecursiveFeatureElimination that has the option to either remove features based on prediction contribution or error contribution.\nFor the error method, the algorithm is:\n\nTrain the model with a set of features.\nCompute the log loss with the validation set.\nLoop through each feature, removing it, retraining the model without it, and computing the new log loss score.\nRemove the feature that, when the model is trained without it, results in the highest log loss score.\nReturn to 1, repeating until a set minimum number of features is reached.\n\ndef get_array_from_df(x):\n    &#039;&#039;&#039;function to convert a data frame to a numpy array.\n \n    parameter:\n        x: data frame\n \n    returns:\n        feature_names_in: feature names\n        x_values: the numpy array\n    &#039;&#039;&#039;\n \n    if isinstance(x, pd.DataFrame):\n        feature_names_in = x.columns.to_numpy()\n        x_values = x.values\n    else:\n        # if this is not a data frame, assign feature names and convert to a numpy array\n        feature_names_in = np.array([f&quot;X{i}&quot; for i in range(x.shape[0])])\n        x_values = np.asarray(x)\n \n    return feature_names_in, x_values\n \n \ndef get_individual_log_loss(y_true, y_proba, eps = 1e-15):\n    &#039;&#039;&#039;function to get the log loss for each observation.\n \n    parameters:\n        y_true: ground truth label\n        y_pred: predicted probability\n \n    returns: log losses\n    &#039;&#039;&#039;\n    y_proba = np.clip(y_proba, eps,1-eps)\n    return - y_true * np.log(y_proba)-(1-y_true)*np.log(1-y_proba)\n \ndef add_metrics_list(metric_dict, y_true, y_pred, y_proba):\n    &#039;&#039;&#039;function to compute some metrics and add them to a dictionary of lists of the metrics.\n \n    parameters:\n        metric_dict: a dictionary with keys for the metric and values are lists of metrics\n        y_true: ground truth label\n        y_pred: predicted label\n        y_proba: predicted probability\n \n    returns:\n        metric_dict: with new scores appended to each list\n    &#039;&#039;&#039;\n    \n    metric_dict[&#039;brier&#039;].append(brier_score_loss(y_true,y_proba))\n    metric_dict[&#039;confusion matrix&#039;].append(confusion_matrix(y_true,y_pred))\n    metric_dict[&#039;log loss&#039;].append(log_loss(y_true,y_proba))\n    metric_dict[&#039;roc auc&#039;].append(roc_auc_score(y_true,y_proba))\n    metric_dict[&#039;accuracy&#039;].append(accuracy_score(y_true,y_pred))\n    metric_dict[&#039;precision&#039;].append(precision_score(y_true,y_pred))\n    metric_dict[&#039;recall&#039;].append(recall_score(y_true,y_pred))\n    metric_dict[&#039;f1&#039;].append(f1_score(y_true,y_pred))\n \n    return metric_dict\n \n \n \nclass RecursiveFeatureElimination:\n    &#039;&#039;&#039;class for recursive feature elimination.\n \n    this is a class that, when the fit method is used, uses recursive feature elimination to select features.\n    the method may either be with prediction or error contributions as determined from SHAP values. the features will be\n    successively eliminated until only the amount set by min_features_to_select remains. output will include a number of\n    metrics compute at each iteration to allow the user to select the best set of features. train data are used for \n    model training, validation data are used to calibrate and compute contributions, and the final performance metrics\n    are done with test data.\n \n    the algorithm is simple, and standard for RFE:\n    1. train model with set of features\n    2. compute feature importance with SHAP values either through prediction or error contribution\n    3. remove the worst scoring feature\n    4. train model with reduced set of features\n    5. compute metrics for later evaluation\n    6. return to 2 until the number of features = min_features_to_select\n \n    attributes:\n        estimator: an estimator\n        min_features_to_select: the number of features to remain after recursive elimination\n        feature_names_in_: names of features\n        ranking_: order in which features were eliminated\n        removal_scores_: the score of the feature removed computed from validation data\n        all_removal_scores_: a list of lists of the scores for each feature that are used for removal in each iteration\n        subsets_: list of all of the subsets of features (column index) used in each iteration\n        supports_: list of boolean lists for features used in each iteration\n        metrics_: dictionary with keys &#039;brier&#039; for brier loss score, &#039;log loss&#039; for log loss score, &#039;confusion matrix&#039;\n        for a confusion matrix, &#039;roc auc&#039; for area under the ROC, &#039;accuracy&#039; for accuracy, &#039;precision&#039; for precision,\n        &#039;recall&#039; for recall, &#039;f1&#039; for F1, computed from test data\n \n    methods:\n        fit(data_dict, verbose, calibrate, n_shap_sample, method)\n            data_dict: a data dictionary with keys X_train, y_train for training data, X_val, y_val for validation\n                        data, and X_test, y_test for test data\n            verbose: bool - whether to print various progress metrics for debugging, default False\n            calibrate: bool - whether to calibrate the estimators, default True\n            n_shap_sample: int - how many samples to use when computing the SHAP values, default 1000\n            method: [&#039;error&#039;,&#039;prediction&#039;] - whether to compute feature importance through error or prediction\n                        contribution, default &#039;error&#039;\n \n&#039;&#039;&#039;\n    def __init__(self, estimator, min_features_to_select = 1):\n        &#039;&#039;&#039;initialize class with estimator and min_features_to_select\n \n        estimator must be some estimator that has both predict() and predict_proba() methods\n \n        min_features_to_select is the number of features to be left after recursive elimination\n        &#039;&#039;&#039;\n        self.estimator = estimator\n        self.min_features_to_select = min_features_to_select\n \n    def fit(self, data_dict, verbose = False, calibrate = True, n_shap_sample = 1000, method = &#039;error&#039;):\n        &#039;&#039;&#039;\n        fit(data_dict, verbose, calibrate, n_shap_sample, method)\n            data_dict: a data dictionary with keys X_train, y_train for training data, X_val, y_val for validation\n                        data, and X_test, y_test for test data\n            verbose: bool - whether to print various progress metrics for debugging, default False\n            calibrate: bool - whether to calibrate the estimators, default True\n            n_shap_sample: int - how many samples to use when computing the SHAP values, default 1000\n            method: [&#039;error&#039;,&#039;prediction&#039;] - whether to compute feature importance through error or prediction\n                        contribution, default &#039;error&#039;\n        &#039;&#039;&#039;\n \n        # dummy check method\n        if method not in [&#039;prediction&#039;,&#039;error&#039;]:\n            raise ValueError(&quot;unknown method, must be either &#039;prediction&#039; or &#039;error&#039;&quot;)\n        \n        # unpack the data dictionary\n        X_train = data_dict[&#039;X_train&#039;]\n        y_train = data_dict[&#039;y_train&#039;]\n        self.feature_names_in_, X_train = get_array_from_df(X_train)\n \n        X_val = data_dict[&#039;X_val&#039;]\n        y_val = data_dict[&#039;y_val&#039;]\n        X_val = get_array_from_df(X_val)[1]\n \n        X_test = data_dict[&#039;X_test&#039;]\n        y_test = data_dict[&#039;y_test&#039;]\n        X_test = get_array_from_df(X_test)[1]\n \n        # get the number of features\n        n_features = X_train.shape[1]\n \n        # add attributes for ranking and support\n        supports = []\n        self.ranking_ = np.repeat(n_features, n_features)\n \n        # get indices for all features\n        current_features = np.arange(n_features)\n \n        # initial fit and calibration of estimator\n        temp = clone(self.estimator)\n        temp.fit(X_train,y_train)\n        if calibrate:\n            temp = CalibratedClassifierCV( FrozenEstimator(temp) ).fit(X_val[:, current_features], y_val )\n \n        # initial predictions\n        pred_prob_val = temp.predict_proba(X_val)[:,1]\n        pred_prob_test = temp.predict_proba(X_test)[:,1]\n        pred_test = temp.predict(X_test)\n        \n        # create the metric dictionary\n        metric_dict = {\n            &#039;brier&#039;:[],\n            &#039;log loss&#039;:[],\n            &#039;confusion matrix&#039;:[],\n            &#039;accuracy&#039;:[],\n            &#039;precision&#039;:[],\n            &#039;recall&#039;:[],\n            &#039;f1&#039;:[],\n            &#039;roc auc&#039;:[]\n        }\n \n        \n        # compute the individual log losses on each observation\n        current_individ_log_loss = log_loss(y_val,pred_prob_val)\n \n        if verbose:\n            print(f&quot;Initial brier loss score: {metric_dict[&#039;brier&#039;]}&quot;)\n            print(pred_prob_test)\n \n        # set the rank\n        rank = 1\n \n        # get the metrics for the initial fit\n        metric_dict = add_metrics_list(metric_dict,y_test,pred_test,pred_prob_test)\n \n        # initialize the list of all subsets\n        all_subsets = [current_features]\n \n        # initialize list of all supports\n        support = np.ones(len(current_features),dtype=bool)\n        if verbose:\n            print(support)\n        supports = [support]\n        all_removal_scores = []\n        worst_scores = []\n        feature_removed = []\n \n        #----- start recursion loop -------------------------------------#\n        while len(current_features)&gt;self.min_features_to_select:\n \n            # get SHAP values\n            X1000 = X_val[:n_shap_sample,current_features]\n            explainer = shap.Explainer(lambda x: temp.predict_proba(x)[:,1],X1000)\n            explained = explainer(X_val[:n_shap_sample,current_features])\n            shap_values = explained.values\n \n            # initialize some lists\n            removal_scores = [] # all of the scores for this iteration\n            subsets = [] # list of all of the subsets for this iteration\n            shap_idx_list = range(len(current_features)) # the current_features has the original column numbers\n                # and for each successive iteration, these are the ones that will be selected from the X data.\n                # however, the SHAP array only has the number of features for the specific iteration, so those\n                # need to be called differently\n \n            # eliminate by which makes the least contribution to prediction\n            if method==&#039;prediction&#039;:\n \n                # prediction contribution found from the mean of the absolute SHAP values for each feature\n                removal_scores = np.mean(np.abs(shap_values),axis=0)\n                if verbose:\n                    print(removal_scores)\n \n            # eliminate by which makes the greatest contribution to error\n            elif method==&#039;error&#039;:\n \n                # loop through all of the features\n                for i in range(len(current_features)):\n \n                    # get the subset of features with one removed\n                    subset = np.delete(current_features,i)\n \n                    # train a new model\n                    temp_estimator = clone(self.estimator)\n                    temp_estimator.fit(X_train[:,subset],y_train)\n                    if calibrate:\n                        temp_calibrated = CalibratedClassifierCV( FrozenEstimator(temp_estimator) ).fit(X_val[:, subset], y_val )\n \n                    # get the log loss (from validation set)\n                    temp_pred = temp_calibrated.predict_proba(X_val[:,subset])[:,1]\n                    removal_scores.append(log_loss(y_val,temp_pred))\n \n            # get the index of the feature to remove\n            best_idx = np.argmin(removal_scores)\n            best_subset = np.delete(current_features,best_idx)\n \n            # add the best subset and score for the removed feature to the running lists\n            all_subsets.append(best_subset)\n            all_removal_scores.append(removal_scores)\n            worst_scores.append(removal_scores[best_idx])\n \n            # remove feature\n            feature_to_remove = current_features[best_idx]\n            feature_removed = self.feature_names_in_[feature_to_remove]\n \n            # assign rank to removed feature\n            self.ranking_[feature_to_remove] = rank\n            rank+=1 # increment rank\n \n            # create boolean for columns\n            support = support.copy()\n            support[feature_to_remove] = False\n            if verbose:\n                print(support)\n            supports.append(support)\n            if verbose:\n                print(supports)\n \n            # reassign current_features\n            current_features = best_subset\n \n            # retrain the model with the new set of features and compute sundry scores\n            temp = clone(self.estimator)\n            temp.fit(X_train[:,current_features], y_train )\n            if calibrate:\n                temp = CalibratedClassifierCV( FrozenEstimator(temp) ).fit(X_val[:, current_features], y_val )\n            \n            pred_prob_val = temp.predict_proba(X_val[:,current_features])[:,1]\n            pred_prob_test = temp.predict_proba(X_test[:,current_features])[:,1]\n            pred_test = temp.predict(X_test[:,current_features])\n            \n            if verbose:\n                print(best_subset)\n                print(pred_prob_test)\n \n            metric_dict = add_metrics_list(metric_dict,y_test,pred_test,pred_prob_test)\n \n            # get the new individual log losses\n            current_individ_log_loss = get_individual_log_loss(y_val,pred_prob_val)\n \n            # tell the user which feature is getting removed\n            print(f&quot;Removing feature {self.feature_names_in_[feature_to_remove]}&quot;)\n \n        # create attributes of informative stuff\n        self.removal_scores_ = worst_scores\n        self.subsets_ = all_subsets\n        self.all_removal_scores_ = all_removal_scores\n        self.supports_ = supports\n        self.metrics_ = metric_dict\n        self.feature_removed_ = feature_removed\n        \n        return self\n        \n        \n \n        \nComparing Prediction and Error Methods of RFE\nInstead of having lengthy lists of arguments for functions or methods, I like to use dictionaries to make them compact.\ndata_dict = {\n    &#039;X_train&#039;:X_train,\n    &#039;y_train&#039;:y_train,\n    &#039;X_val&#039;:X_val,\n    &#039;y_val&#039;:y_val,\n    &#039;X_test&#039;:X_test,\n    &#039;y_test&#039;:y_test\n}\nThe next loop will train RFE objects where the RFE is based on error contribution (RFE_losses) and prediction contribution (RFE_preds). This is for demonstration purposes.\n# set up empty dictionaries; each key will be one of the estimators; RFE_losses will use error method, RFE_preds will use\n# prediction method\nRFE_losses = {}\nRFE_preds = {}\n \n \nfor estimator in estimators.keys():\n    \n    print(&#039;doing RFE on &#039;+estimator)\n \n    print(&#039;loss&#039;)\n    RFE_loss = RecursiveFeatureElimination(estimators[estimator])\n    RFE_loss.fit(data_dict,n_shap_sample=500);\n    RFE_losses[estimator] = RFE_loss\n    \n    print(&#039;prediction&#039;)\n    RFE_pred = RecursiveFeatureElimination(estimators[estimator])\n    RFE_pred.fit(data_dict,n_shap_sample=500,method=&#039;prediction&#039;);\n    RFE_preds[estimator] = RFE_pred\nfg,ax = plt.subplots(1,2,figsize=(12,6))\ncolors = [&#039;k&#039;,&#039;b&#039;,&#039;g&#039;,&#039;r&#039;]\nlines = []\nfor i, estimator in enumerate(RFE_losses.keys()):\n    line, = ax[0].plot(RFE_losses[estimator].metrics_[&#039;brier&#039;][0:16],colors[i],label=estimator)\n    lines.append(line)\n    ax[0].plot(RFE_preds[estimator].metrics_[&#039;brier&#039;][0:16],colors[i]+&#039;--&#039;)#,label=estimator+&quot; prediction&quot;)\n    \n    ax[1].plot(RFE_losses[estimator].metrics_[&#039;f1&#039;][0:16],colors[i])\n    ax[1].plot(RFE_preds[estimator].metrics_[&#039;f1&#039;][0:16],colors[i]+&#039;--&#039;)\n#ax.legend()\nfg.suptitle(&#039;Performance of Models and RFE Methods&#039;)\nfg.supxlabel(&#039;solid line by error, dashed line by prediction&#039;)\n \nax[0].set_title(&#039;Brier Scores Loss&#039;)\nax[0].set_ylabel(&#039;Brier score loss&#039;)\nax[0].set_xlabel(&#039;iteration&#039;)\n \nax[1].set_title(&#039;F1 Scores&#039;)\nax[1].set_ylabel(&#039;F1 score&#039;)\nax[1].set_xlabel(&#039;iteration&#039;)\nax[1].legend(handles = lines)\nfg.tight_layout()\nplt.show()\n\nHere we can see comparison of two significant scores: Brier loss and F1. For the F1 scores, it looks as though for the first ten feature removals, the error method outperforms the prediction method. This is also true for Brier score loss for the ensemble methods, although not for the logistic regression or neural network. We can get quantities by looking at what the best of each of the metrics computed in the object fitting is, and how the prediction and error methods compare.\nloss_metrics = [&#039;brier&#039;,&#039;log loss&#039;]\ngain_metrics = [&#039;roc auc&#039;,&#039;accuracy&#039;,&#039;precision&#039;,&#039;recall&#039;,&#039;f1&#039;]\n \n \nmetrics_table_dict = {}\nfor_index = []\n \ndef get_metrics_for_list(metrics_list,RFE_object,metric_names_list,metric_type=&#039;loss&#039;):\n \n    metric_list_numeric = []\n    for metric in metric_names_list:\n        if metric_type==&#039;loss&#039;:\n            measure = np.min(RFE_object.metrics_[metric])\n            metrics_list.append(f&quot;{measure:.04f}&quot;)\n            metric_list_numeric.append(measure)\n            idx = np.argmin(RFE_object.metrics_[metric])\n        elif metric_type==&#039;gain&#039;:\n            measure = np.max(RFE_object.metrics_[metric])\n            metrics_list.append(f&quot;{measure:.04f}&quot;)\n            metric_list_numeric.append(measure)\n            idx = np.argmax(RFE_object.metrics_[metric])\n        metrics_list.append(f&quot;{len(RFE_object.subsets_[idx]):.0f}&quot;)\n        metric_list_numeric.append(np.nan)\n \n    return metric_list_numeric\n \nfor estimator in estimators.keys():\n \n    pred_metrics_list = []\n    loss_metrics_list = []\n    \n    temp1 = get_metrics_for_list(pred_metrics_list,RFE_preds[estimator],loss_metrics,metric_type=&#039;loss&#039;)\n    temp2 = get_metrics_for_list(loss_metrics_list,RFE_losses[estimator],loss_metrics,metric_type=&#039;loss&#039;)\n    metrics_diff1 = [f&quot;{temp1[i] - temp2[i]:.04f}&quot; for i in range(len(temp1))]\n \n    temp1 = get_metrics_for_list(pred_metrics_list,RFE_preds[estimator],gain_metrics,metric_type=&#039;gain&#039;)\n    temp2 = get_metrics_for_list(loss_metrics_list,RFE_losses[estimator],gain_metrics,metric_type=&#039;gain&#039;)\n    metrics_diff2 = [f&quot;{temp1[i] - temp2[i]:.04f}&quot; for i in range(len(temp1))]\n \n    metrics_table_dict[estimator+&#039; by error&#039;] = loss_metrics_list\n    metrics_table_dict[estimator+&#039; by prediction&#039;] = pred_metrics_list\n    metrics_table_dict[estimator+&#039; prediction - error&#039;] = metrics_diff1+metrics_diff2\n \nall_metrics = loss_metrics+gain_metrics\nfor metric in all_metrics:\n    for_index.extend([metric,&#039;n features &#039;+metric])\n \n \nmetrics_table = pd.DataFrame(metrics_table_dict,index=for_index).replace(&#039;nan&#039;,&#039;&#039;)\n \n \nmetrics_table\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      logistic regression by error\n      logistic regression by prediction\n      logistic regression prediction - error\n      random forest by error\n      random forest by prediction\n      random forest prediction - error\n      ada boost by error\n      ada boost by prediction\n      ada boost prediction - error\n      multilayer perceptron by error\n      multilayer perceptron by prediction\n      multilayer perceptron prediction - error\n    \n  \n  \n    \n      brier\n      0.0181\n      0.0181\n      -0.0001\n      0.0378\n      0.0425\n      0.0046\n      0.0425\n      0.0435\n      0.0010\n      0.0080\n      0.0080\n      -0.0000\n    \n    \n      n features brier\n      25\n      21\n      \n      23\n      25\n      \n      24\n      18\n      \n      22\n      24\n      \n    \n    \n      log loss\n      0.0847\n      0.0847\n      0.0000\n      0.1437\n      0.1586\n      0.0149\n      0.1588\n      0.1604\n      0.0016\n      0.0437\n      0.0441\n      0.0004\n    \n    \n      n features log loss\n      22\n      21\n      \n      23\n      25\n      \n      24\n      25\n      \n      22\n      25\n      \n    \n    \n      roc auc\n      0.9912\n      0.9912\n      0.0000\n      0.9824\n      0.9794\n      -0.0030\n      0.9797\n      0.9793\n      -0.0004\n      0.9928\n      0.9928\n      0.0000\n    \n    \n      n features roc auc\n      22\n      20\n      \n      21\n      24\n      \n      24\n      25\n      \n      25\n      25\n      \n    \n    \n      accuracy\n      0.9784\n      0.9778\n      -0.0006\n      0.9530\n      0.9432\n      -0.0098\n      0.9450\n      0.9446\n      -0.0004\n      0.9914\n      0.9910\n      -0.0004\n    \n    \n      n features accuracy\n      16\n      25\n      \n      23\n      23\n      \n      14\n      18\n      \n      24\n      25\n      \n    \n    \n      precision\n      0.9732\n      0.9713\n      -0.0019\n      0.9403\n      0.9287\n      -0.0116\n      0.9154\n      0.9144\n      -0.0010\n      0.9898\n      0.9891\n      -0.0007\n    \n    \n      n features precision\n      23\n      25\n      \n      23\n      22\n      \n      23\n      18\n      \n      21\n      24\n      \n    \n    \n      recall\n      0.9590\n      0.9543\n      -0.0047\n      0.9012\n      0.8844\n      -0.0168\n      0.9032\n      0.8978\n      -0.0054\n      0.9839\n      0.9825\n      -0.0013\n    \n    \n      n features recall\n      16\n      21\n      \n      17\n      25\n      \n      15\n      18\n      \n      24\n      25\n      \n    \n    \n      f1\n      0.9635\n      0.9624\n      -0.0012\n      0.9193\n      0.9023\n      -0.0169\n      0.9071\n      0.9061\n      -0.0010\n      0.9855\n      0.9848\n      -0.0007\n    \n    \n      n features f1\n      16\n      21\n      \n      23\n      21\n      \n      14\n      18\n      \n      24\n      25\n      \n    \n  \n\n\nThis table is the best value (smallest for loss scores, largest for everything else) and the number of features in the iteration with that score, with a column that is simply the difference between the prediction method and error method. For the loss scores, if the difference is positive, the error method is performing better. For the others, if the difference is negative, the error method is performing better. And here we see in the majority of cases, the error method yields a better value.\nIn the end, what we see is that the neural network trained with the error method is the clear winner for this data set."},"portfolio-1/index":{"slug":"portfolio-1/index","filePath":"portfolio-1/index.md","title":"Classification Model With Numerical Features","links":["portfolio-1/1-construction","portfolio-1/data-exploration","portfolio-1/modeling","portfolio-1/feature-selection","portfolio-1/conclusion"],"tags":[],"content":"The first example I would like to give is the creation of a classification model using numerical regressors. I will walk through a skeleton of the standard techniques for approaching a data set, first doing initial exploration, then going into model selection. Instead of this simply being a straightforward (and boring!) stroll, I am also going to take up a few topics I rarely find discussed elsewhere and explore them a bit.\nThere are three fairly novel topics herein, all of which I found on Towards Data Science posts. One is the use of a mutual information coefficient based on relative entropy to measure associations, instead of something more common such as a correlation coefficient. Another is the use of calibration on the probabilistic predictions. Finally, an exploration of whether or not it is more effective to use the contribution of a feature to error rather than prediction for feature selection.\n\nConstruction of the Data Set\nData Exploration\nModeling\nModel Feature Selection\nConclusion\n"},"portfolio-1/modeling":{"slug":"portfolio-1/modeling","filePath":"portfolio-1/modeling.md","title":"3. Modeling","links":[],"tags":[],"content":"Now we can begin with modeling. We will first see whether predictive modeling is even a viable option. That is, we will see whether the categories have some amount of separation in a reduced dimensional space. Then, if it is an option, we proceed first with some individual looks at models (which will include calibration of the probabilities), and conclude with feature selection to come up with a viable model.\n\nSeparability of target category\nCreating the Train, Validation, and Test Sets\nInitial Modeling Exploration\nCalibration of Probabilities\n\nSeparability of target category\nFor this step, we need some method for reducing the dimensions to see easily. I will use the simple principle component analysis (PCA) method.\n# do the PCA\nX = PCA(n_components=2).fit_transform(data)\n \n# make the plot\nfg,ax = plt.subplots(1,1,figsize=(4,4))\nnegscat = ax.scatter(X[data[&#039;target&#039;]==0,0],X[data[&#039;target&#039;]==0,1],c=&#039;black&#039;,s=1,label=&#039;negative&#039;);\nposscat = ax.scatter(X[data[&#039;target&#039;]==1,0],X[data[&#039;target&#039;]==1,1],c=&#039;red&#039;,s=1,label=&#039;positive&#039;);\nax.set_title(&quot;Principle Component Analysis&quot;);\nax.legend(handles=[negscat,posscat]);\n\nSo there is some degree of separability, thus modeling is possible.\nCreating the Train, Validation, and Test Sets\nNow we’re going to split the set into three parts: The training set, a validation set, and a test set. The validation set will be used for calibrating probabilities and computing feature importance. The test set will be used to evaluate the quality of the model.\n# split the data frame into three parts, first by taking a random sample of 35,000 from the total\nTrainSet = data.sample(35000, random_state = 42)\n \n# next we drop those and select 10,000 for our validation set\ntemp = data.drop(TrainSet.index)\nValSet = temp.sample(10000, random_state = 42)\n \n# the test set is whatever remains\nTestSet = temp.drop(ValSet.index)\n \n# a quick function for splitting each of the sets into X and y\ndef get_XY(df):\n    X = df[df.columns[:-1]]\n    y = df[df.columns[-1]]\n    return X,y\n \nX_train, y_train = get_XY(TrainSet)\nX_val, y_val = get_XY(ValSet)\nX_test, y_test = get_XY(TestSet)\nInitial Modeling Exploration\nFor the sake of simplicity, I will only try out four classifiers: logistic regression, random forest, Ada boost, and a multilayer perceptron neural network. Prior experimentation showed that for the logistic regression, the features needed to be scaled, so I create a pipeline with StandardScaler().\nI create a dictionary with the models, which I then loop through first to train them, and then to create and display confusion matrices. Here I train the models with the training set, and use the validation set for the confusion matrices.\n# create a dictionary of the estimators\nestimators = { &#039;logistic regression&#039;:\n    Pipeline(\n        [\n            (&#039;scaler&#039;,StandardScaler()),\n            (&#039;clf&#039;,LogisticRegression())\n        ]\n    ),\n    &#039;random forest&#039;:RandomForestClassifier(random_state = 42, max_depth = 5, max_features = 1),\n    &#039;ada boost&#039;:AdaBoostClassifier(random_state = 42),\n    &#039;multilayer perceptron&#039;:MLPClassifier( alpha = 1, max_iter = 1000, random_state = 42 )\n}\n \n# create empty dictionary that will contain the confusion matrices\nconfusion_matrices = {}\n \n# loop through estimators, train them, and get the confusion matrix for each\nfor estimator in estimators.keys():\n    cloned = clone(estimators[estimator])\n    cloned.fit(X_train,y_train)\n    y_pred = cloned.predict(X_val)\n    confusion_matrices[estimator] = confusion_matrix( y_val, y_pred, normalize = &#039;all&#039;)\n \n# create plots\nfg, ax = plt.subplots(2,2)\n \nlabels = [&#039;negative&#039;,&#039;positive&#039;]\n \ncount = 0\nfor i in range(2):\n    for j in range(2):\n        temp = list(estimators.keys())[count]\n        ConfusionMatrixDisplay(confusion_matrices[temp],display_labels = labels).plot(ax=ax[i,j])\n        ax[i,j].set(title=temp)\n        count+=1\nax[0,0].set(xlabel=None)\nax[0,1].set(xlabel=None)\nax[0,1].set(ylabel=None)\nax[1,1].set(ylabel=None)\n \nfg.tight_layout()\nplt.show()\n\nUnsurprisingly, given the dataset was constructed to be fairly clean, the models preform quite well.\nCalibration of Probabilities\nNext I would like to calibrate the probabilities. Many of the estimator predictions made with predict_proba are uncalibrated. Besides accurate probabilities being nice to have, the method of feature selection based on error contribution below uses the cross entropy or log loss, which requires calibrated probabilities.\nFirst I will set up some utility functions.\n \ndef get_scores(y_true, y_pred, y_proba ):\n    &#039;&#039;&#039; a function that will quickly compute the brier loss, log loss, and F1 scores for binary classes.\n     \n    parameters:\n        y_true: ground truth labels\n        y_pred: predicted labels\n        y_proba: predicted probabilities\n \n    returns:\n        score: a dictionary that has the elements:\n                    &#039;textstr&#039;: a text string that can be used in a plot\n                    &#039;brier&#039;: brier loss score\n                    &#039;logloss&#039;: log loss score\n                    &#039;f1&#039;: F1 score\n                    &#039;&#039;&#039;\n    \n    # dummy checks\n    if len(np.unique(y_true))&gt;2 or len(np.unique(y_pred))&gt;2:\n        raise ValueError(&quot;must be binary categories&quot;)\n \n    if np.any( y_proba&lt;0 ) or np.any( y_proba&gt;1 ):\n        raise ValueError(&quot;probabilities must be on the interval [0,1]&quot;)\n \n    # compute the scores \n    Bscore = brier_score_loss(y_true, y_proba)\n    LogLoss = log_loss(y_true, y_proba)\n    F1 = f1_score( y_true, y_pred)\n \n    # create a text string\n    textstr = &#039;\\n&#039;.join((\n        f&quot;Brier score loss: {Bscore:0.3f}&quot;,\n        f&quot;Log loss: {LogLoss:0.3f}&quot;,\n        f&quot;F1 score: {F1:0.3f}&quot;#,\n    ))\n \n    # create the data dictionary to return\n    scores = {&#039;textstr&#039;:textstr,&#039;brier&#039;:Bscore,&#039;logloss&#039;:LogLoss,&#039;f1&#039;:F1}\n \n    return scores\n \n \ndef plot_reliability_curve(y_true, y_pred, y_proba, ax):\n    &#039;&#039;&#039;a function to plot the reliability curve or calibration curve.\n \n    parameters:\n        y_true: ground truth labels\n        y_pred: predicted labels\n        y_proba: predicted probabilities\n        ax: axis to plot on\n \n    returns nothing\n    &#039;&#039;&#039;\n \n    # create a line for perfect calibration\n    ax.plot([0,1], [0,1], &#039;b--&#039;, label = &#039;perfect calibration&#039;)\n \n    # get the scores\n    scores = get_scores(y_true, y_pred, y_proba)\n \n    # create the text box\n    props = dict(boxstyle = &#039;round&#039;, facecolor = &#039;wheat&#039;, alpha = 0.5)\n    ax.text(0.05, 0.95, scores[&#039;textstr&#039;], transform = ax.transAxes, fontsize = 10, verticalalignment = &#039;top&#039;, bbox = props)\n \n    # get the calibration curve and plot it\n    fraction_of_positives, mean_predicted_values = calibration_curve(y_true, y_proba, n_bins = 100)\n    ax.plot(mean_predicted_values, fraction_of_positives)\n \ndef get_predictions( estimator, X_test ):\n    &#039;&#039;&#039;a function to get the predictions of labels and probabilities from an estimator.\n \n    parameters:\n        estimator: an estimator to be used to mkae predictions\n        X_test: a set to make the predictions on\n \n    return:\n        y_proba: predicted probabilities\n        y_pred: predicted labels\n        &#039;&#039;&#039;\n    y_proba = estimator.predict_proba(X_test)[:,1]\n    y_pred = estimator.predict(X_test)\n    return y_proba, y_pred\n \nNext we will do the calibrations and create a set of plots. The left plots will be uncalibrated predictions, and the right will be calibrated ones.\n# set up a dictionary that will hold all of the calibrated estimators\ncalibrated = {}\n \n# create the figure\nfg, ax = plt.subplots(4,2,figsize=(10,10))\n \n# loop to create plots and calibrated estimators\nfor i, estimator in enumerate(estimators.keys()):\n \n    # get predictions for the uncalibrated estimator\n    cloned = clone(estimators[estimator]).fit(X_train, y_train)\n    y_proba, y_pred = get_predictions(cloned, X_test)\n \n    # plot the reliability curve for the uncalibrated estimator\n    plot_reliability_curve( y_test, y_pred, y_proba, ax[i,0] )\n    ax[i,0].set_ylabel(&quot;Fraction of positives&quot;)\n    ax[i,0].set_title(estimator+&quot; uncalibrated&quot;)\n    \n    # calibrate the estimator\n    calibrated[estimator] = CalibratedClassifierCV( FrozenEstimator(cloned) ).fit(X_val, y_val)\n \n    # get predictions for the calibrated estimator\n    y_proba, y_pred = get_predictions(calibrated[estimator], X_test)\n \n    # plot the reliability curve for the calibrated estimator\n    plot_reliability_curve( y_test, y_pred, y_proba, ax[i,1] )\n    ax[i,1].set_title(estimator+&quot; calibrated&quot;)\n \n# set axis labels and plot\nax[3,0].set_xlabel(&quot;Mean predicted value&quot;)\nax[3,1].set_xlabel(&quot;Mean predicted value&quot;)\n \nfg.tight_layout()\nplt.show()\n\nHere we see dramatic changes when calibrating the ensemble methods, and minor ones for the logistic regression and neural network. We can also see that the losses are dramatically lower for the neural network than the other methods. Based on this, we might simply drop the others and go forward with only the neural network, but for the purposes of illustration I would like to continue with all of them."}}